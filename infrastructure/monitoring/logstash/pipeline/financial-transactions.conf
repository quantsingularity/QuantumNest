# Logstash Pipeline Configuration for Financial Transaction Processing
# financial-transactions.conf

input {
  # Kafka input for real-time transaction logs
  kafka {
    bootstrap_servers => ["kafka.quantumnest.internal:9092"]
    topics => ["financial-transactions", "audit-logs", "security-events"]
    group_id => "logstash-financial"
    consumer_threads => 4
    decorate_events => true
    codec => json
    security_protocol => "SSL"
    ssl_truststore_location => "/usr/share/logstash/config/certs/kafka.truststore.jks"
    ssl_truststore_password => "${KAFKA_TRUSTSTORE_PASSWORD}"
    ssl_keystore_location => "/usr/share/logstash/config/certs/kafka.keystore.jks"
    ssl_keystore_password => "${KAFKA_KEYSTORE_PASSWORD}"
    ssl_key_password => "${KAFKA_KEY_PASSWORD}"
  }

  # Beats input for application logs
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
    ssl_key => "/usr/share/logstash/config/certs/logstash.key"
    ssl_certificate_authorities => ["/usr/share/logstash/config/certs/ca.crt"]
    ssl_verify_mode => "force_peer"
  }

  # HTTP input for webhook logs
  http {
    port => 8080
    ssl => true
    ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
    ssl_key => "/usr/share/logstash/config/certs/logstash.key"
    codec => json
    additional_codecs => {
      "application/json" => "json"
      "text/plain" => "plain"
    }
  }
}

filter {
  # Parse timestamp
  date {
    match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSS", "yyyy-MM-dd'T'HH:mm:ss.SSSZ" ]
    target => "@timestamp"
  }

  # Add common fields
  mutate {
    add_field => {
      "environment" => "production"
      "platform" => "quantumnest"
      "processed_at" => "%{+yyyy-MM-dd'T'HH:mm:ss.SSSZ}"
    }
  }

  # Process financial transaction logs
  if [kafka][topic] == "financial-transactions" {
    # Parse transaction data
    json {
      source => "message"
      target => "transaction"
    }

    # Validate required fields
    if ![transaction][transaction_id] {
      mutate {
        add_tag => ["_transaction_id_missing"]
      }
    }

    if ![transaction][amount] {
      mutate {
        add_tag => ["_amount_missing"]
      }
    }

    # Convert amount to float
    mutate {
      convert => { "[transaction][amount]" => "float" }
    }

    # Categorize transaction amounts
    if [transaction][amount] {
      if [transaction][amount] >= 10000 {
        mutate {
          add_field => { "transaction_category" => "large" }
          add_tag => ["large_transaction"]
        }
      } else if [transaction][amount] >= 1000 {
        mutate {
          add_field => { "transaction_category" => "medium" }
        }
      } else {
        mutate {
          add_field => { "transaction_category" => "small" }
        }
      }
    }

    # Detect suspicious patterns
    if [transaction][amount] >= 9999.99 and [transaction][amount] <= 10000.01 {
      mutate {
        add_tag => ["suspicious_structuring"]
        add_field => { "alert_type" => "potential_structuring" }
      }
    }

    # Geolocation enrichment
    if [transaction][ip_address] {
      geoip {
        source => "[transaction][ip_address]"
        target => "geoip"
        database => "/usr/share/logstash/GeoLite2-City.mmdb"
      }
    }

    # PII masking for compliance
    mutate {
      gsub => [
        "[transaction][account_number]", "(\d{4})\d+(\d{4})", "\1****\2",
        "[transaction][card_number]", "(\d{4})\d+(\d{4})", "\1****\2",
        "[transaction][ssn]", "(\d{3})\d+(\d{4})", "\1**\2"
      ]
    }

    # Calculate transaction hash for integrity
    fingerprint {
      source => ["[transaction][transaction_id]", "[transaction][amount]", "[transaction][timestamp]"]
      target => "transaction_hash"
      method => "SHA256"
      key => "${TRANSACTION_HASH_KEY}"
    }

    # Add compliance tags
    mutate {
      add_tag => ["pci_dss", "sox", "aml"]
      add_field => {
        "data_classification" => "confidential"
        "retention_period" => "2555"  # 7 years in days
      }
    }
  }

  # Process audit logs
  if [kafka][topic] == "audit-logs" {
    json {
      source => "message"
      target => "audit"
    }

    # Enrich with user information
    if [audit][user_id] {
      elasticsearch {
        hosts => ["https://elasticsearch.quantumnest.internal:9200"]
        index => "user-directory"
        query => "user_id:%{[audit][user_id]}"
        fields => {
          "department" => "user_department"
          "role" => "user_role"
          "manager" => "user_manager"
        }
        ssl => true
        ca_file => "/usr/share/logstash/config/certs/ca.crt"
        ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
        ssl_key => "/usr/share/logstash/config/certs/logstash.key"
      }
    }

    # Classify audit events
    if [audit][action] in ["login", "logout", "password_change"] {
      mutate {
        add_field => { "audit_category" => "authentication" }
      }
    } else if [audit][action] in ["create", "update", "delete"] {
      mutate {
        add_field => { "audit_category" => "data_modification" }
      }
    } else if [audit][action] in ["view", "download", "export"] {
      mutate {
        add_field => { "audit_category" => "data_access" }
      }
    }

    # Add compliance tags
    mutate {
      add_tag => ["soc2", "iso27001", "audit_trail"]
      add_field => {
        "data_classification" => "restricted"
        "retention_period" => "2555"  # 7 years
      }
    }
  }

  # Process security events
  if [kafka][topic] == "security-events" {
    json {
      source => "message"
      target => "security"
    }

    # Threat intelligence enrichment
    if [security][ip_address] {
      translate {
        source => "[security][ip_address]"
        target => "threat_intel"
        dictionary_path => "/usr/share/logstash/threat_intel.yml"
        fallback => "unknown"
      }
    }

    # Security event classification
    if [security][event_type] in ["intrusion_attempt", "malware_detected", "policy_violation"] {
      mutate {
        add_field => { "severity" => "high" }
        add_tag => ["security_incident"]
      }
    }

    # Add security tags
    mutate {
      add_tag => ["security", "incident_response"]
      add_field => {
        "data_classification" => "restricted"
        "retention_period" => "2555"
      }
    }
  }

  # Remove sensitive fields before output
  mutate {
    remove_field => ["[transaction][raw_card_data]", "[audit][password]", "[security][private_key]"]
  }
}

output {
  # Main Elasticsearch cluster
  if "financial-transactions" in [kafka][topic] {
    elasticsearch {
      hosts => ["https://elasticsearch.quantumnest.internal:9200"]
      index => "financial-transactions-%{+YYYY.MM.dd}"
      template_name => "financial-transactions"
      template => "/usr/share/logstash/templates/financial-transactions.json"
      template_overwrite => true
      ssl => true
      ca_file => "/usr/share/logstash/config/certs/ca.crt"
      ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
      ssl_key => "/usr/share/logstash/config/certs/logstash.key"
      user => "logstash_writer"
      password => "${LOGSTASH_WRITER_PASSWORD}"
      retry_on_conflict => 3
      action => "index"
    }
  }

  if "audit-logs" in [kafka][topic] {
    elasticsearch {
      hosts => ["https://elasticsearch.quantumnest.internal:9200"]
      index => "audit-logs-%{+YYYY.MM.dd}"
      template_name => "audit-logs"
      template => "/usr/share/logstash/templates/audit-logs.json"
      template_overwrite => true
      ssl => true
      ca_file => "/usr/share/logstash/config/certs/ca.crt"
      ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
      ssl_key => "/usr/share/logstash/config/certs/logstash.key"
      user => "logstash_writer"
      password => "${LOGSTASH_WRITER_PASSWORD}"
    }
  }

  if "security-events" in [kafka][topic] {
    elasticsearch {
      hosts => ["https://elasticsearch.quantumnest.internal:9200"]
      index => "security-events-%{+YYYY.MM.dd}"
      template_name => "security-events"
      template => "/usr/share/logstash/templates/security-events.json"
      template_overwrite => true
      ssl => true
      ca_file => "/usr/share/logstash/config/certs/ca.crt"
      ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
      ssl_key => "/usr/share/logstash/config/certs/logstash.key"
      user => "logstash_writer"
      password => "${LOGSTASH_WRITER_PASSWORD}"
    }
  }

  # High-priority alerts to dedicated index
  if "security_incident" in [tags] or "large_transaction" in [tags] or "suspicious_structuring" in [tags] {
    elasticsearch {
      hosts => ["https://elasticsearch.quantumnest.internal:9200"]
      index => "alerts-%{+YYYY.MM.dd}"
      ssl => true
      ca_file => "/usr/share/logstash/config/certs/ca.crt"
      ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
      ssl_key => "/usr/share/logstash/config/certs/logstash.key"
      user => "logstash_writer"
      password => "${LOGSTASH_WRITER_PASSWORD}"
    }
  }

  # Dead letter queue for failed documents
  if "_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
    elasticsearch {
      hosts => ["https://elasticsearch.quantumnest.internal:9200"]
      index => "failed-logs-%{+YYYY.MM.dd}"
      ssl => true
      ca_file => "/usr/share/logstash/config/certs/ca.crt"
      ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
      ssl_key => "/usr/share/logstash/config/certs/logstash.key"
      user => "logstash_writer"
      password => "${LOGSTASH_WRITER_PASSWORD}"
    }
  }

  # Metrics output to Prometheus
  http {
    url => "https://prometheus-pushgateway.quantumnest.internal:9091/metrics/job/logstash"
    http_method => "post"
    format => "message"
    message => "logstash_processed_events_total %{[@metadata][processed_count]}"
    ssl_verification_mode => "full"
    ssl_certificate_authorities => ["/usr/share/logstash/config/certs/ca.crt"]
    ssl_certificate => "/usr/share/logstash/config/certs/logstash.crt"
    ssl_key => "/usr/share/logstash/config/certs/logstash.key"
  }

  # Debug output (remove in production)
  # stdout {
  #   codec => rubydebug {
  #     metadata => true
  #   }
  # }
}

